{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVjj3IhwNIw8Vf6tV89YNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuditDubey01/Sentimental-Analysis-Project/blob/main/SentimentalAnalysis_on_Sentiment140dataset_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Twitter Tweets Sentiment Classification\n",
        "\n",
        "* Sentiment classification is the automated process of identifying opinions in text and labeling them as positive, negative, or neutral, based on the emotions customers express within them.\n",
        "\n",
        "* Here since the labelled data that we have only has 2 classes, 0-Negative, 4-Positive (later encoded as 1) we will perform a binary classification\n"
      ],
      "metadata": {
        "id": "TBvNPxRVQhFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries and dependencies"
      ],
      "metadata": {
        "id": "lhy-txaIX_no"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvhDExgxQbIj"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "tqdm.pandas()\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "lemm = WordNetLemmatizer()\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.rcParams['figure.figsize'] = (20,8)\n",
        "plt.rcParams['font.size'] = 18\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Import\n",
        "* Reading and sampling the data\n",
        "\n",
        "* Since the data contains 1.6m records, it is not possible to create a feature vector of this size with atleast 1000-2000 reasonable features\n",
        "\n",
        "* Therefore we randomly sample out 100000 records for our training and validation"
      ],
      "metadata": {
        "id": "oud0OI_RRNXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"../input/sentiment140/training.1600000.processed.noemoticon.csv\",encoding = \"latin\",header=None)\n",
        "data = data.iloc[:,[5,0]]\n",
        "data.columns = ['Text','Class']\n",
        "data = data.sample(102000)\n",
        "data.reset_index(drop=True,inplace=True)\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "D8MaHantQcWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Cleaning**\n",
        "\n",
        "\n",
        "\n",
        "1. Removal of extra spaces, lines and indentations\n",
        "2. Removal of special characters like punctuations and emojis\n",
        "3. Conversion of text into lower case\n",
        "4. Tokenization\n",
        "5. Filtering and removal of stopwords\n",
        "5. Lemmatization\n",
        "\n",
        "\n",
        "\n",
        "While other steps are fairly easy to understand, given below is an example of lemmatization (Stemming can also be used as an alternative, but lets say we are more interested in the intelligible root words so we will go with Lemmatization)\n",
        "\n",
        "__Example of Lemmatization__\n",
        "<img src='https://user.oc-static.com/upload/2020/10/22/16033589274175_lemmatization%20example%2001.png'>"
      ],
      "metadata": {
        "id": "zOJ2qWHjRtsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.remove('not')\n",
        "\n",
        "def text_cleaning(x):\n",
        "    \n",
        "    text = re.sub('\\s+\\n+', ' ', x)\n",
        "    text = re.sub('[^a-zA-Z0-9]', ' ', x)\n",
        "    text = re.sub('^\\S', ' ', x)\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "    \n",
        "    text = [lemm.lemmatize(word, \"v\") for word in text if word not in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "vlnd1qivQceb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Clean Text'] = data['Text'].progress_apply(text_cleaning)"
      ],
      "metadata": {
        "id": "Am9Ypj43Qchv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "_XtCy7TTQckb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "jLpbbI5dSQGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[data['Class']==4,'Class'] = 1\n",
        "data['Class'].value_counts()"
      ],
      "metadata": {
        "id": "0FNdJBHKSQIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA and Visualization\n",
        "* Exploring class count distribution\n",
        "* Frequently occuring words in negative and positive tweets\n",
        "* Sentence Length Distribution Analysis"
      ],
      "metadata": {
        "id": "R7czPGyQSU4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=data,y='Class')\n",
        "plt.title(\"Class Count Distribution\")\n",
        "plt.show()  "
      ],
      "metadata": {
        "id": "XrJTQMGFSQPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = data[data['Class']==1]['Clean Text'].tolist()\n",
        "negative = data[data['Class']==0]['Clean Text'].tolist()"
      ],
      "metadata": {
        "id": "51H3vAzUSQST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordClouds and Barplots of Frequently Occuring words"
      ],
      "metadata": {
        "id": "T2_pIyw3Sjl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(max_words=1500, width=600, background_color='black').generate(\" \".join(positive))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title(\"Positive Tweets Wordcloud\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mrafY6uLShP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "y = []\n",
        "for key,value in wordcloud.words_.items():\n",
        "    x.append(key)\n",
        "    y.append(value)\n",
        "    if len(x) == 15:\n",
        "        break\n",
        "sns.barplot(x=x,y=y,color='black')\n",
        "plt.title(\"Normalized Count of Top-15 Frequent Words with Positive Sentiments\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Normalized Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_vr9Ki1hShTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(max_words=1500, width=600, background_color='black').generate(\" \".join(negative))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title(\"Negative Tweets Wordcloud\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OnPCMZS6SpbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "y = []\n",
        "for key,value in wordcloud.words_.items():\n",
        "    x.append(key)\n",
        "    y.append(value)\n",
        "    if len(x) == 15:\n",
        "        break\n",
        "sns.barplot(x=x,y=y,color='black')\n",
        "plt.title(\"Normalized Count of Top-15 Frequent Words with Negative Sentiments\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Normalized Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g9hZ5W6FSpdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['sentence_length'] = data['Clean Text'].progress_apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "SwR1tlvXSpgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=data,y='sentence_length',x='Class')\n",
        "plt.title(\"IQR Analysis of Sentence Lengths\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RhLCMXQTSpjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['sentence_length'].describe()"
      ],
      "metadata": {
        "id": "qTVx7ywNSwLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = data.tail(2000)\n",
        "data = data.iloc[:100000,:]"
      ],
      "metadata": {
        "id": "MuFvqGiTSwPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction and Modelling\n",
        "* Extracting features using TF-iDF extraction method\n",
        "* We will avoid using Count Vectorizer over here since the document is very large and certain words might get too high values/importance based on their presence in the entire document corpus, and since we want to focus on the relative presence of different kinds of words in the text, TF-iDF will be a more preferred choice of feature extraction"
      ],
      "metadata": {
        "id": "bgEKEaAdS41I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://miro.medium.com/max/1200/1*V9ac4hLVyms79jl65Ym_Bw.jpeg'>\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vDfO5P0eTB8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "RBkZrkwBSwhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=2500)\n",
        "X = vectorizer.fit_transform(data['Clean Text'].values.tolist()).toarray()\n",
        "y = data['Class'].values"
      ],
      "metadata": {
        "id": "mXZRLBXiTKDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ngram_range=(1,2):  vectorizer should consider both unigrams and bigrams in the text, which means that it will consider individual words as well as pairs of words.\n",
        "\n",
        "* max_features=2500 : vectorizer should only consider the 2500 most frequently occurring words in the text.\n",
        "\n",
        "* X = vectorizer.fit_transform(data['Clean Text'].values.tolist()).toarray() \n",
        ": Applies the vectorize,  The fit_transform() method applies the vectorizer to the data and converts it into a sparse matrix of TF-IDF features. The toarray() method is then used to convert the sparse matrix into a dense matrix, which can be used as input to a machine learning model. The resulting X matrix contains the TF-IDF features for each tweet in the dataset."
      ],
      "metadata": {
        "id": "TPIeDlepW__o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "75AuBdNxTKAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score"
      ],
      "metadata": {
        "id": "q_vrhHXITKGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train,y_train)\n",
        "    y_pred_tr = model.predict(X_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"--------------------Training Performance---------------------\")\n",
        "    print(classification_report(y_train,y_pred_tr))\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(\"--------------------Testing Performance----------------------\")\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    \n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred),cmap='viridis',annot=True,fmt='.4g',\n",
        "            xticklabels=['Negative','Positive'],yticklabels=['Negative','Positive'])\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Actual Class')\n",
        "    plt.show()\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(y_test,  y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    plt.plot(fpr,tpr,label=\"CNN Model, auc=\"+str(auc),lw=2)\n",
        "    plt.plot([0, 1], [0, 1], color=\"orange\", lw=2, linestyle=\"--\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DMr3eUhCTKIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Navie Bayes"
      ],
      "metadata": {
        "id": "XMuhXHLZTcYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB()\n",
        "model_train(model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "yx_gxYxwTZ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression\n",
        "* Max iterations is set to 1000, so that the model can run for more iterations for convergence\n",
        "* With its default number of iterations (100) the model fails to converge"
      ],
      "metadata": {
        "id": "evpCNY3pTgr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model_train(model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "7N4U0pO9TaP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest\n",
        "* Uses 100 decision trees to perform bagging and produce and ensemble result\n",
        "* Max depth is set to 15 to avoid overfitting on the training set\n",
        "* Max features have been set sqrt of the number of features (performs slightly better than log and auto)"
      ],
      "metadata": {
        "id": "qlcJk42ETs2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100,max_depth=15,max_features='sqrt')\n",
        "model_train(model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "7h3ncBhITpJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model | F1-Score | Accuracy | AUC Score |\n",
        "| --- | --- | --- | --- |\n",
        "| Naive Bayes | 0.75 | 0.75 | 0.75 |\n",
        "| __Logistic Regression__ | 0.76 | 0.76 | 0.758 |\n",
        "| Naive Bayes | 0.70 | 0.70 | 0.69 |\n",
        "\n",
        "__Inference:__ Based on the Accuracy, F1-Score and AUC Score Logistic Regression performs the best on the validation set"
      ],
      "metadata": {
        "id": "mnAuU4TNT4az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Post Feature Extraction Analysis\n",
        "* Here we will apply PCA just for the sake of visualizing how the data looks on a 2D plane\n",
        "* Since the number of records and features are very high it is difficult to look at the correlation and reason with it to apply PCA"
      ],
      "metadata": {
        "id": "eN2f-m2jT_xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "fbiGMBVkTpPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_train)\n",
        "variance_explained = np.cumsum(pca.explained_variance_ratio_)\n",
        "pcs = range(1,len(variance_explained)+1)"
      ],
      "metadata": {
        "id": "J15poj8XUIYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysing the Variance Explanantion\n",
        "* 2 components will explain even less than 15% of the total variance of our feature extracted data, but will try to visualize it regardless\n",
        "* It will still require over 2000 components to represent more than 90% of the variance of the data, seems like there is not much correlation in the feature extracted data, which is justified since it is a sparse matrix"
      ],
      "metadata": {
        "id": "Sc03akMEUMqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(pcs,variance_explained)\n",
        "plt.title(\"PCs Explained Variance\")\n",
        "plt.xlabel(\"Principal Components\")\n",
        "plt.ylabel(\"Variance Explanation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7a99BQl-UIbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Set"
      ],
      "metadata": {
        "id": "X3fKqF4KUWq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vis_df = pd.DataFrame(X_pca[:,:2])\n",
        "vis_df['class'] = y_train\n",
        "\n",
        "sns.scatterplot(x=0,y=1,data=vis_df,hue='class')\n",
        "plt.title(\"Visualization of TF-iDF embeddings in 2D-Plane using PCA\")\n",
        "plt.xlabel(\"First PC\")\n",
        "plt.ylabel(\"Second PC\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AxGku4bmUIf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation Set"
      ],
      "metadata": {
        "id": "YUOarN6GUeDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pca = pca.transform(X_test)\n",
        "vis_df = pd.DataFrame(X_test_pca[:,:2])\n",
        "vis_df['class'] = y_test\n",
        "\n",
        "sns.scatterplot(x=0,y=1,data=vis_df,hue='class')\n",
        "plt.title(\"Visualization of TF-iDF embeddings in 2D-Plane using PCA\")\n",
        "plt.xlabel(\"First PC\")\n",
        "plt.ylabel(\"Second PC\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c1AnNc8xUIjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**: No clear distinctice pattern as expected, the non linearities however can be captured using feed forward neural nets or some other non linear models"
      ],
      "metadata": {
        "id": "77_9Z3qoUjtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Pipeline\n",
        "- Lets put the above feature extraction and modelling steps inside a pipeline\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*ah8eEa2j4NULlMUts6UFNA.png'>"
      ],
      "metadata": {
        "id": "QPxMU6cnUorM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Set"
      ],
      "metadata": {
        "id": "xw6Wfsj_Utv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train,y_train)\n",
        "pipe = Pipeline([('feature_extraction', vectorizer), ('logit', model)])"
      ],
      "metadata": {
        "id": "WONPIJDvUIno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = test['Clean Text'].tolist()\n",
        "test_label = test['Class']"
      ],
      "metadata": {
        "id": "lC_vQGBOUIrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe.predict(test_input)"
      ],
      "metadata": {
        "id": "GCPGzxRjUIts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_label,outputs))"
      ],
      "metadata": {
        "id": "AxxFOqgvUIxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**: *The model gives same performance on the previously unseen non preprocessed test set as well*"
      ],
      "metadata": {
        "id": "Y6q4eeFPU4tR"
      }
    }
  ]
}